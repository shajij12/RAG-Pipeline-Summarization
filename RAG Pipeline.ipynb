{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346d91e9-95f3-45e8-80bc-4f4dc6e2a3c6",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">RAG Pipeline - Summarization</h1>\n",
    "<h4 align=\"center\">Shaji Joseph</h4>\n",
    "<h4 align=\"center\"> CSCI 685: Computational Lingustics</h4>\n",
    "<h4 align=\"center\"> CWID: 50394653</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2927ae6-86ed-4b80-882b-922aa1ab0a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ff6b6f0-dc14-48a3-ade4-57fbaec85c99",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Dependencies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f77b7c-586b-422d-800c-bacdb19654a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaji.joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\shaji.joseph\\AppData\\Roaming\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\shaji.joseph\\AppData\\Roaming\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\shaji.joseph\\AppData\\Roaming\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "                                                            # ##############################\n",
    "                                                            #  Environment Configuration   #\n",
    "                                                            ################################\n",
    "import os\n",
    "# Configure CUDA environment variable for debugging GPU operations\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "                                                            ################################\n",
    "                                                            #       Web Search & Scrape    #\n",
    "                                                            ################################\n",
    "from duckduckgo_search import DDGS  # For web searching\n",
    "import requests  # For making HTTP requests to websites\n",
    "from bs4 import BeautifulSoup  # For web scraping and HTML parsing\n",
    "import re  # For regular expression operations\n",
    "\n",
    "                                                            ################################\n",
    "                                                            #       Text Processing        #\n",
    "                                                            ################################\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For text chunking\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "nltk.download('punkt')  # Download NLTK tokenizer model\n",
    "from nltk.tokenize import sent_tokenize  # For sentence tokenization\n",
    "\n",
    "                                                            ################################\n",
    "                                                            #       Embeddings & NLP       #\n",
    "                                                            ################################\n",
    "from sentence_transformers import SentenceTransformer  # For generating embeddings\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification  # For NLP tasks\n",
    "import spacy  # For advanced NLP tasks (POS tagging,NER, etc)\n",
    "\n",
    "                                                            ################################\n",
    "                                                            #       Vector Search          #\n",
    "                                                            ################################\n",
    "import faiss  # For similarity search and indexing\n",
    "\n",
    "                                                            ################################\n",
    "                                                            #       Utilities              #\n",
    "                                                            ################################\n",
    "import numpy as np  # For numerical operations\n",
    "import tkinter as tk  # For GUI creation\n",
    "import warnings  # For warning control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d675470-5ade-4dca-b2fb-e6d25254e5cc",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">RAG Pipeline Class</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7337b6-a2d9-4779-a02b-0d206101aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG (Retrieval-Augmented Generation) pipeline class\n",
    "class RAG_pipeline():\n",
    "    # Initializes the RAG pipeline with the user's query.\n",
    "    def __init__(self, query):\n",
    "        # Store the user's query as an instance variable \n",
    "        self.query = query\n",
    "############################################################################################        \n",
    "# Search the web using DuckDuckGo API\n",
    "    def web_search(self, query):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        with DDGS() as ddgs:\n",
    "            # Get top 3 text results for the query\n",
    "            results = list(ddgs.text(query, max_results=3))\n",
    "            # Extract and return URLs from results\n",
    "            return [result['href'] for result in results]\n",
    "############################################################################################            \n",
    "# Fetch webpage content and clean the text by Downloading HTML content, Extracting paragraph text, and Removing noise (parentheses, brackets, extra whitespace)\n",
    "    def scrape_clean(self, url):\n",
    "        # Fetch webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(\"Article content extracted.\")\n",
    "        # Extract all paragraph elements\n",
    "        paragraphs = soup.find_all('p')\n",
    "        # Join paragraphs into single text\n",
    "        txt = ' '.join([p.get_text() for p in paragraphs])\n",
    "        # Clean text content\n",
    "        txt = re.sub(r'\\([^)]*\\)', '', txt)\n",
    "        cln = re.sub(r'\\[.*?\\]', '', txt)\n",
    "        cln = re.sub(r'\\s+', ' ', cln).strip()\n",
    "        return cln      \n",
    "############################################################################################        \n",
    "# Split text into chunks of 256 characters with 20 characters overlap\n",
    "    def chunking(self, text):\n",
    "        print('Chunking...')\n",
    "        # Initialize the text splitter with Ideal chunk length in characters (256) and Context-preserving overlap (20 chars)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\n",
    "        # Perform the actual text splitting\n",
    "        return text_splitter.split_text(text)\n",
    "############################################################################################        \n",
    "    # Generates vector embeddings for text chunks using a pre-trained sentence transformer model.\n",
    "    def generate_embeddings(self, chunks):\n",
    "        print('Generating embeddings...')\n",
    "        # Initialize the pre-trained sentence transformer model 'all-MiniLM-L6-v2' Lightweight model\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # Generate embeddings for all chunks. Convert each chunk to lowercase for case-insensitive processing\n",
    "        return model.encode([c.lower() for c in chunks], normalize_embeddings=True)\n",
    "############################################################################################\n",
    " # Calculate similarity scores between query and document chunks, ranking and returning the most relevant text portions.\n",
    "    def similarity_metric(self, embeddings, query_embedding, chunks):\n",
    "        print('Calculating similarity scores...')\n",
    "        # Get embedding dimension from the shape of embeddings matrix\n",
    "        dimension = embeddings.shape[1]\n",
    "        # Initialize FAISS index using Inner Product\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        # Add document embeddings to the search index\n",
    "        index.add(np.array(embeddings))\n",
    "        # Perform similarity search\n",
    "        # query embedding - The vector representation of the user's query.Retrieve top 16 most similar chunks\n",
    "        # D - Array of distances (not used here)\n",
    "        # I - Array of indices for the most similar chunks\n",
    "        D, I = index.search(np.array(query_embedding), k=16)\n",
    "        # Formulate the final text by joining the top ranked chunks in order of relevance\n",
    "        return ' '.join([chunks[i] for i in I[0]])\n",
    "############################################################################################\n",
    " # Generate summary from long text using BART model\n",
    "    def summarize(self, longtext):\n",
    "        print('Generating summary...')\n",
    "        # Initialize the tokenizer to handle truncation\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        # Tokenize the input text with truncation\n",
    "        inputs = tokenizer(longtext, return_tensors=\"pt\", truncation=True, max_length=1024, padding=\"max_length\")\n",
    "        # Decode the tokenized input back to text after truncation\n",
    "        truncatedtext = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "        # Initialize the summarizer pipeline\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        # Generate the summary using the model\n",
    "        summary = summarizer(truncatedtext, max_length=900, min_length=150, do_sample=False)\n",
    "        # Return the summary text\n",
    "        return summary[0]['summary_text']\n",
    "##############################################################################################\n",
    "# Paraphrase text using a pre-trained T5-based model for paraphrasing\n",
    "\n",
    "    def paraphrase(self, text):\n",
    "        print('Generating paraphrases...')\n",
    "        # Initialize the paraphrasing pipeline\n",
    "        paraphraser = pipeline(\"text2text-generation\", model=\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "        \n",
    "        # Split the text into sentences for better paraphrasing\n",
    "        sentences = sent_tokenize(text)\n",
    "        paraphrased_sentences = []\n",
    "        # Loop over each sentence and generate a paraphrased version\n",
    "        for sentence in sentences:    \n",
    "            prompt = \"paraphrase: \" + sentence + \" </s>\"\n",
    "            # Generate a single paraphrased output\n",
    "            paraphrases = paraphraser(prompt, num_beams=15, num_return_sequences=3, max_length=256, early_stopping=True,temperature=0.9, top_k=50, repetition_penalty=3.0)\n",
    "            paraphrased_sentences.append(paraphrases[0]['generated_text'])\n",
    "        # Join all paraphrased sentences into a single text block and return\n",
    "        return ' '.join(paraphrased_sentences)\n",
    "################################################################################################\n",
    " # Execute the full RAG pipeline: search, scrape, chunk, embed, and summarize\n",
    "    def generate_summary(self):\n",
    "        #  Web Search - Get relevant URLs using the query\n",
    "        urls = self.web_search(self.query)\n",
    "        # Web Scraping - Extract and clean text from the first URL\n",
    "        clean_webtext = self.scrape_clean(urls[0])\n",
    "        # Chunking - Split text into manageable pieces\n",
    "        chunks = self.chunking(clean_webtext)\n",
    "        # Generate embeddings for document chunks\n",
    "        doc_embeddings = self.generate_embeddings(chunks)\n",
    "        # Generate embedding for the original query\n",
    "        query_embedding = self.generate_embeddings([self.query])\n",
    "        # Retrieve most relevant chunks using similarity search\n",
    "        long_text = self.similarity_metric(doc_embeddings, query_embedding, chunks)\n",
    "        \n",
    "        # Generate initial summary from retrieved text\n",
    "        summary = self.summarize(long_text)\n",
    "        \n",
    "        # Augment with paraphrasing - Refine the summary for better readability\n",
    "        paraphrased_summary = self.paraphrase(summary)\n",
    "        \n",
    "        return paraphrased_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e071f-b010-49d6-a255-b12d8725b5e9",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Formating Summary</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee641f3-b68b-4cde-87dd-f89307e94889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaji.joseph\\AppData\\Roaming\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "                            ###############################################\n",
    "                            # Proper Noun Capitalizer using NER and Spacy #\n",
    "                            ###############################################\n",
    "\n",
    "# spaCy for part-of-speech tagging\n",
    "# HuggingFace pipeline for Named Entity Recognition (NER)\n",
    "nlp, ner = spacy.load(\"en_core_web_sm\"), pipeline(\"ner\", model=\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\n",
    "\n",
    "def capitalize(text):\n",
    "    #  Extract all named entities (PER, ORG, LOC) from text\n",
    "    ents = {w.lower() for e in ner(text) if (t:=e.get('entity_group',e.get('entity',''))) in ['PER','ORG','LOC'] \n",
    "            # regex to extract individual words\n",
    "            for w in re.findall(r'\\b\\w+\\b', e['word'].lower())}\n",
    "            # Identify all prepositions (ADP) in text using spaCy\n",
    "    preps = {t.text.lower() for t in nlp(text) if t.pos_ == \"ADP\"}\n",
    "    # Process each word in the text\n",
    "    words, fixed, sentstart = text.split(), [], True\n",
    "    for i, w in enumerate(words):\n",
    "        # Skip and preserve pure punctuation/non-word tokens\n",
    "        if not (m := re.match(r'^(\\W*)(\\w+)(\\W*)$', w)):\n",
    "            fixed.append(w)\n",
    "            sentstart = any(p in w for p in ['.','!','?'])\n",
    "            continue\n",
    "        # Split words into prefix (punctuation), core word, and suffix (punctuation)    \n",
    "        p, c, s = m.groups()\n",
    "        # # Lowercase for case-insensitive checks\n",
    "        lc = c.lower()\n",
    "        # sentence starters\n",
    "        c = (c.capitalize() if lc in ents or (sentstart and lc not in preps) \n",
    "             # Force lowercase prepositions\n",
    "             else lc if lc in preps else c)\n",
    "        # words ending with sentence-ending punctuations\n",
    "        sentstart = any(p in s for p in ['.','!','?']) or (i>0 and words[i-1][-1] in ['.','!','?'])\n",
    "        # Reconstruct word with original punctuation\n",
    "        fixed.append(f\"{p}{c}{s}\")\n",
    "    \n",
    "    return ' '.join(fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4de82-7e02-4d16-8b66-03d9534b6ab3",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Final Summary</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3034a30-a27b-42c2-841a-c42a8cd6b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the user input and executes the full summarization pipeline\n",
    "def show_input(event=None):\n",
    "    # Disable Submit button and get user input\n",
    "    submitbutton.config(state=tk.DISABLED)\n",
    "    # Get and clean input string\n",
    "    userinput = entry.get().strip() \n",
    "    \n",
    "    # Show processing summary warning\n",
    "    resultlabel.config(text=f\"Please Wait.... Processing Summary for: {userinput}...\", fg=\"black\")\n",
    "    # Force GUI update\n",
    "    window.update_idletasks()  \n",
    "    if not userinput.strip():\n",
    "        resultlabel.config(text=\"Please enter a valid query.\", fg=\"red\")\n",
    "        return\n",
    "    # Initialize RAG pipeline with user query\n",
    "    pipeline_obj = RAG_pipeline(userinput)\n",
    "    # Generate raw summary through the full RAG process\n",
    "    raw_summary = pipeline_obj.generate_summary()\n",
    "    # Split summary into sentences\n",
    "    sentences = sent_tokenize(raw_summary)\n",
    "    formatted_sentences = []\n",
    "    # Format each sentence (capitalize first letter)\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        if s:\n",
    "            s = s[0].upper() + s[1:] if len(s) > 1 else s.upper()\n",
    "            formatted_sentences.append(s)\n",
    "            \n",
    "    # Group sentences into paragraphs (3 sentences per paragraph)\n",
    "    paragraphs = [\n",
    "        ' '.join(formatted_sentences[i:i+3])\n",
    "        for i in range(0, len(formatted_sentences), 3)\n",
    "    ]\n",
    "    formatted_output = '\\n\\n'.join(paragraphs)\n",
    "    # Apply proper noun capitalization to the formatted output\n",
    "    finaloutput = capitalize(formatted_output)\n",
    "    # Formulate and show the final output in the result label\n",
    "    resultlabel.config(text=finaloutput, anchor='w', justify='left', fg=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9a652-ac85-4b22-a547-a1fd87be5b79",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">GUI Layout</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fee9e1-9d33-4a56-8202-9a0ed6b91049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content extracted.\n",
      "Chunking...\n",
      "Generating embeddings...\n",
      "Generating embeddings...\n",
      "Calculating similarity scores...\n",
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 900, but your input_length is only 562. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=281)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating paraphrases...\n",
      "Article content extracted.\n",
      "Chunking...\n",
      "Generating embeddings...\n",
      "Generating embeddings...\n",
      "Calculating similarity scores...\n",
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 900, but your input_length is only 632. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=316)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating paraphrases...\n"
     ]
    }
   ],
   "source": [
    "# Set up the main application window\n",
    "window = tk.Tk()\n",
    "window.title(\"Summarizer\")\n",
    "window.geometry(\"600x400\")\n",
    "\n",
    "# Add GUI elements: labels, entry fields, and buttons\n",
    "prompt = tk.Label(window, text=\"Enter a Query String\", font=('Helvetica', 12))\n",
    "prompt.pack(pady=5)\n",
    "\n",
    "# Create and place the text entry field\n",
    "entry = tk.Entry(window, width=40, font=('Helvetica', 11))\n",
    "entry.pack(pady=5)\n",
    "\n",
    "# Set the cursor focus and re-enable the submit button\n",
    "entry.focus_force()\n",
    "entry.bind(\"<Return>\", lambda event: show_input())\n",
    "entry.bind(\"<Key>\", lambda e: submitbutton.config(state=tk.NORMAL))\n",
    "\n",
    "# Create a frame to hold action buttons\n",
    "buttonframe = tk.Frame(window)\n",
    "buttonframe.pack(pady=10)\n",
    "\n",
    "# Submit button configuration\n",
    "submitbutton = tk.Button(buttonframe, text=\"Submit\", command=show_input, bg=\"#4CAF50\", fg=\"white\", padx=10)\n",
    "submitbutton.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "# Close button configuration\n",
    "close_button = tk.Button(buttonframe, text=\"Close\", command=window.destroy, bg=\"#f44336\", fg=\"white\", padx=10)\n",
    "close_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "# Create and configure the results label\n",
    "resultlabel = tk.Label(window, text=\"\", wraplength=560, justify='left', anchor='w', bg=\"#f8f9fa\", font=('Helvetica', 11), relief=tk.SUNKEN, padx=10, pady=10)\n",
    "resultlabel.pack(pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Start the main event loop - keeps the window responsive\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
